{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b42cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://ebbnflow.tistory.com/246"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f32cea",
   "metadata": {},
   "source": [
    "# 0. Basic\n",
    "\n",
    "기초적인 전처리,\n",
    "html tag 제거(크롤링한 데이터일경우)\n",
    "숫자, 영어, 특수문자 등 필요하지 않은 언어 제거\n",
    "Lowercasing\n",
    "\"@%*=()/+ 와 같은 punctuation(문장부호) 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2bade9",
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n",
    "punct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', }\n",
    "\n",
    "\n",
    "def clean_punc(text, punct, mapping):\n",
    "    for p in mapping:\n",
    "        text = text.replace(p, mapping[p])\n",
    "    \n",
    "    for p in punct:\n",
    "        text = text.replace(p, f' {p} ')\n",
    "    \n",
    "    specials = {'\\u200b': ' ', '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': ''}\n",
    "    for s in specials:\n",
    "        text = text.replace(s, specials[s])\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be492518",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def clean_text(texts):\n",
    "    corpus = []\n",
    "    for i in range(0, len(texts)):\n",
    "        review = re.sub(r'[@%\\\\*=()/~#&\\+á?\\xc3\\xa1\\-\\|\\.\\:\\;\\!\\-\\,\\_\\~\\$\\'\\\"]', '',str(texts[i])) #remove punctuation\n",
    "        review = re.sub(r'\\d+','', str(texts[i]))# remove number\n",
    "        review = review.lower() #lower case\n",
    "        review = re.sub(r'\\s+', ' ', review) #remove extra space\n",
    "        review = re.sub(r'<[^>]+>','',review) #remove Html tags\n",
    "        review = re.sub(r'\\s+', ' ', review) #remove spaces\n",
    "        review = re.sub(r\"^\\s+\", '', review) #remove space from start\n",
    "        review = re.sub(r'\\s+$', '', review) #remove space from the end\n",
    "        corpus.append(review)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a0c9f3",
   "metadata": {},
   "source": [
    "# 1. Tokenize\n",
    " \n",
    "자연어 처리에서는 텍스트를 '토큰 단위'로 나눈다.\n",
    "'I am a student' 영어에서 토큰은 단순히 띄어쓰기를 해서 보면 되지만, 'i', 'am', 'a', 'student'\n",
    "한국어에서 띄어쓰기는 텍스트 의미를 구분하는데 큰 영향을 준다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283daaa4",
   "metadata": {},
   "source": [
    "애초에 모든 공백을 없앤 후, 문맥에 따라 띄어쓴 문장을 만드는 것이 좋은 방법\n",
    "-> '너무기대를 안했나봐' -> '너무기대를안했나봐' -> '너무 기대를 안 했나 봐'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae36c474",
   "metadata": {},
   "source": [
    "PyKoSpacing을 사용\n",
    "https://github.com/haven-jeon/PyKoSpacing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd07fa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install git+https://github.com/haven-jeon/PyKoSpacing.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252a96b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pykospacing import spacing\n",
    "\n",
    "\n",
    "print(spacing(\"김형호영화시장분석가는'1987'의네이버영화정보네티즌10점평에서언급된단어들을지난해12월27일부터올해1월10일까지통계프로그램R과KoNLP패키지로텍스트마이닝하여분석했다.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accbe3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "// 결과\n",
    "\n",
    "김형호 영화시장 분석가는 '1987'의 네이버 영화 정보 네티즌 10점 평에서 언급된 단어들을 지난해 12월 27일부터 올해 1월 10일까지 통계 프로그램 R과 KoNLP 패키지로 텍스트마이닝하여 분석했다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afc3eb1",
   "metadata": {},
   "source": [
    "## kss: 문장 분리 > 한국어 문장분리 파이썬 라이브러리\n",
    "\n",
    "문장 분리의 경우 형태소 분석으로 종결어미를 구분한다던지, 문장의 CRF(Conditional Random Field) 결과로 판단하는 방법이 있다. kss - Korean Sentence Splitter 은 pip으로 빠르게 설치가 가능하며, 형태소분석기(KoNLPy)가 성능이 비슷비슷한 것을 감안했을 때, kss로 성능을 높일 수도 있다.\n",
    "kss는 정교한 패턴 기반의 문장 분리기라는 장점과 함께 통계 기반에 비해 월등히 뛰어난 속도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c77704",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install kss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46b8806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kss\n",
    "\n",
    "\n",
    "s = \"회사 동료 분들과 다녀왔는데 분위기도 좋고 음식도 맛있었어요 다만, 강남 토끼정이 강남 쉑쉑버거 골목길로 쭉 올라가야 하는데 다들 쉑쉑버거의 유혹에 넘어갈 뻔 했답니다 강남역 맛집 토끼정의 외부 모습.\"\n",
    "for sent in kss.split_sentences(s):\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9492030",
   "metadata": {},
   "outputs": [],
   "source": [
    "#// 결과\n",
    "#회사 동료 분들과 다녀왔는데 분위기도 좋고 음식도 맛있었어요\n",
    "#다만, 강남 토끼정이 강남 쉑쉑버거 골목길로 쭉 올라가야 하는데 다들 쉑쉑버거의 유혹에 넘어갈 뻔 했답니다\n",
    "#강남역 맛집 토끼정의 외부 모습."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe763b53",
   "metadata": {},
   "source": [
    "# 2. Spell Check\n",
    "py-hanspell : 네이버 맞춤법 검사기를 이용한 파이썬용 한글 맞춤법 검사 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5daa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install py-hanspell\n",
    "#필요한 의존 라이브러리는 requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08319e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#사용법1 - dict로 출력\n",
    "from hanspell import spell_checker\n",
    "\n",
    "\n",
    "result = spell_checker.check(u'안녕 하세요. 저는 한국인 입니다. 이문장은 한글로 작성됬습니다.')\n",
    "result.as_dict()  # dict로 출력\n",
    "{'checked': '안녕하세요. 저는 한국인입니다. 이 문장은 한글로 작성됐습니다.',\n",
    " 'errors': 4,\n",
    " 'original': '안녕 하세요. 저는 한국인 입니다. 이문장은 한글로 작성됬습니다.',\n",
    " 'result': True,\n",
    " 'time': 0.07065701484680176,\n",
    " 'words': {'안녕하세요.': 2,\n",
    "           '저는': 0,\n",
    "           '한국인입니다.': 2,\n",
    "           '이': 2,\n",
    "           '문장은': 2,\n",
    "           '한글로': 0,\n",
    "           '작성됐습니다.': 1}}\n",
    ">>> result\n",
    "Checked(result=True, original='안녕 하세요. 저는 한국인 입니다. 이문장은 한글로 작성됬습니다.', checked='안녕하세요. 저는 한국인입니다. 이 문장은 한글로 작성됐습니다.', errors=4, words=OrderedDict([('안녕하세요.', 2), ('저는', 0), ('한국인입니다.', 2), ('이', 2), ('문장은', 2), ('한글로', 0), ('작성됐습니다.', 1)]), time=0.10472893714904785)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddea8b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#사용법2 - list로 주고받기\n",
    "from hanspell import spell_checker\n",
    "\n",
    "\n",
    "spell_checker.check([u'안녕 하세요.', u'저는 한국인 입니다.'])\n",
    "[Checked(result=True, original='안녕 하세요.', checked='안녕하세요.', errors=1, words=OrderedDict([('안녕하세요.', 2)]), time=0.03297615051269531),\n",
    " Checked(result=True, original='저는 한국인 입니다.', checked='저는 한국인입니다.', errors=1, words=OrderedDict([('저는', 0), ('한국인입니다.', 2)]), time=0.029018878936767578)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b713585b",
   "metadata": {},
   "source": [
    "### parameters\n",
    "\n",
    "result : 맞춤법 검사 성공여부\n",
    "original : 검사 전의 문장\n",
    "checked : 맞춤법 검사 후의 문장\n",
    "errors : 맞춤법 오류 수\n",
    "time : 총 요청 시간\n",
    "words : Checked.words\n",
    "- words 부분은 교정된 최종 문장을 공백으로 나눈(split) 결과\n",
    "- 결과는 key가 단어, value가 CheckResult를 나타냄\n",
    " 아래 참고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6930db",
   "metadata": {},
   "source": [
    "for key, value in result.words.items():\n",
    "...    print(key, value)\n",
    "안녕하세요. 2\n",
    "저는 0\n",
    "한국인입니다. 2\n",
    "이 2\n",
    "문장은 2\n",
    "한글로 0\n",
    "작성됐습니다. 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e21b313",
   "metadata": {},
   "source": [
    "### CheckResult \n",
    "아래 코드로 import 한 후, 비교에 사용할 수 있는 상수\n",
    "from hanspell.constants import CheckResult\n",
    "\n",
    ".PASSED\n",
    "맞춤법 검사 결과 문제가 없는 단어 또는 구절\n",
    "\n",
    ".WRONG_SPELLING\n",
    "맞춤법에 문제가 있는 단어 또는 구절\n",
    "\n",
    ".WRONG_SPACING\n",
    "띄어쓰기에 문제가 있는 단어 또는 구절\n",
    "\n",
    ".AMBIGUOUS\n",
    "표준어가 의심되는 단어 또는 구절\n",
    "\n",
    ".STATISTICAL_CORRECTION\n",
    "통계적 교정에 따른 단어 또는 구절"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ed0172",
   "metadata": {},
   "source": [
    "### 반복되는 이모티콘이나 자모 ㅎㅎㅎㅎ 하하하하 ㅋㅋㅋ 같은 것을 nomalize하는 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3584e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install soynlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd014aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from soynlp.normalizer import *\n",
    "print(repeat_normalize('와하하하하하하하하하핫', num_repeats=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca8d6c4",
   "metadata": {},
   "source": [
    "### 외래어 사전을 다운받아 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fe7b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=1RNYpLE-xbMCGtiEHIoNsCmfcyJP3kLYn\" > /dev/null\n",
    "!curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=1RNYpLE-xbMCGtiEHIoNsCmfcyJP3kLYn\" -o confused_loanwords.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6895f666",
   "metadata": {},
   "outputs": [],
   "source": [
    "lownword_map = {}\n",
    "lownword_data = open('/content/confused_loanwords.txt', 'r', encoding='utf-8')\n",
    "\n",
    "lines = lownword_data.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    miss_spell = line.split('\\t')[0]\n",
    "    ori_word = line.split('\\t')[1]\n",
    "    lownword_map[miss_spell] = ori_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58176e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_check_text(texts):\n",
    "    corpus = []\n",
    "    for sent in texts:\n",
    "        spaced_text = spacing(sent)\n",
    "        spelled_sent = spell_checker.check(sent)\n",
    "        checked_sent = spelled_sent.checked\n",
    "        normalized_sent = repeat_normalize(checked_sent)\n",
    "        for lownword in lownword_map:\n",
    "            normalized_sent = normalized_sent.replace(lownword, lownword_map[lownword])\n",
    "        corpus.append(normalized_sent)\n",
    "    return corpus\n",
    "    \n",
    "spell_preprocessed_corpus = spell_check_text(basic_preprocessed_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ff1fe4",
   "metadata": {},
   "source": [
    "# 3. Pos Tag\n",
    "konlpy의 형태소 분석기의 성능은 비슷함. 그나마 mecab이 가장 뛰어난 것으로 알려져있음 (하지만 mecab은 윈도우에서 사용하지 못함)\n",
    "Python기반의 형태소 분석기 중 카카오의 Khaiii라는 오픈 소스가 성능이 가장 좋음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dac44cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#설치\n",
    "!git clone https://github.com/kakao/khaiii.git\n",
    "!pip install cmake\n",
    "!mkdir build\n",
    "!cd build && cmake /content/khaiii\n",
    "!cd /content/build/ && make all\n",
    "!cd /content/build/ && make resource\n",
    "!cd /content/build && make install\n",
    "!cd /content/build && make package_python\n",
    "!pip install /content/build/package_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21542d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from khaiii import KhaiiiApi\n",
    "api = KhaiiiApi()\n",
    "\n",
    "test_sents = [\"나도 모르게 사버렸다.\"]\n",
    "\n",
    "for sent in test_sents:\n",
    "    for word in api.analyze(sent):\n",
    "        for morph in word.morphs:\n",
    "            print(morph.lex + '/' + morph.tag)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07b8965",
   "metadata": {},
   "outputs": [],
   "source": [
    "// 결과\n",
    "\n",
    "나/NP\n",
    "도/JX\n",
    "모르/VV\n",
    "게/EC\n",
    "사/VV\n",
    "아/EC\n",
    "버리/VX\n",
    "었/EP\n",
    "다/EF\n",
    "./SF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf489490",
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_tags = ['NNG', 'NNP', 'NNB', 'VV', 'VA', 'VX', 'MAG', 'MAJ', 'XSV', 'XSA']\n",
    "\n",
    "def pos_text(texts):\n",
    "    corpus = []\n",
    "    for sent in texts:\n",
    "        pos_tagged = ''\n",
    "        for word in api.analyze(sent):\n",
    "            for morph in word.morphs:\n",
    "                if morph.tag in significant_tags:\n",
    "                    pos_tagged += morph.lex + '/' + morph.tag + ' '\n",
    "        corpus.append(pos_tagged.strip())\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3609207",
   "metadata": {},
   "outputs": [],
   "source": [
    "// 결과 예시\n",
    "\n",
    "제임스/NNP 얼/NNG 지/NNP 미/NNG 카터/NNP 주니/NNG 어/NNP 민주당/NNP 출신/NNG 미국/NNP 번/NNB 대통령/NNG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af647200",
   "metadata": {},
   "source": [
    "# 4. Stemming\n",
    "동사를 원형으로 복원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2158fd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = re.compile('[가-힣A-Za-z0-9]+/NN. [가-힣A-Za-z0-9]+/XS.')\n",
    "p2 = re.compile('[가-힣A-Za-z0-9]+/NN. [가-힣A-Za-z0-9]+/XSA [가-힣A-Za-z0-9]+/VX')\n",
    "p3 = re.compile('[가-힣A-Za-z0-9]+/VV')\n",
    "p4 = re.compile('[가-힣A-Za-z0-9]+/VX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64a22a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming_text(text):\n",
    "    corpus = []\n",
    "    for sent in text:\n",
    "        ori_sent = sent\n",
    "        mached_terms = re.findall(p1, ori_sent)\n",
    "        for terms in mached_terms:\n",
    "            ori_terms = terms\n",
    "            modi_terms = ''\n",
    "            for term in terms.split(' '):\n",
    "                lemma = term.split('/')[0]\n",
    "                tag = term.split('/')[-1]\n",
    "                modi_terms += lemma\n",
    "            modi_terms += '다/VV'\n",
    "            ori_sent = ori_sent.replace(ori_terms, modi_terms)\n",
    "        \n",
    "        mached_terms = re.findall(p2, ori_sent)\n",
    "        for terms in mached_terms:\n",
    "            ori_terms = terms\n",
    "            modi_terms = ''\n",
    "            for term in terms.split(' '):\n",
    "                lemma = term.split('/')[0]\n",
    "                tag = term.split('/')[-1]\n",
    "                if tag != 'VX':\n",
    "                    modi_terms += lemma\n",
    "            modi_terms += '다/VV'\n",
    "            ori_sent = ori_sent.replace(ori_terms, modi_terms)\n",
    "\n",
    "        mached_terms = re.findall(p3, ori_sent)\n",
    "        for terms in mached_terms:\n",
    "            ori_terms = terms\n",
    "            modi_terms = ''\n",
    "            for term in terms.split(' '):\n",
    "                lemma = term.split('/')[0]\n",
    "                tag = term.split('/')[-1]\n",
    "                modi_terms += lemma\n",
    "            if '다' != modi_terms[-1]:\n",
    "                modi_terms += '다'\n",
    "            modi_terms += '/VV'\n",
    "            ori_sent = ori_sent.replace(ori_terms, modi_terms)\n",
    "\n",
    "        mached_terms = re.findall(p4, ori_sent)\n",
    "        for terms in mached_terms:\n",
    "            ori_terms = terms\n",
    "            modi_terms = ''\n",
    "            for term in terms.split(' '):\n",
    "                lemma = term.split('/')[0]\n",
    "                tag = term.split('/')[-1]\n",
    "                modi_terms += lemma\n",
    "            if '다' != modi_terms[-1]:\n",
    "                modi_terms += '다'\n",
    "            modi_terms += '/VV'\n",
    "            ori_sent = ori_sent.replace(ori_terms, modi_terms)\n",
    "        corpus.append(ori_sent)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103e3643",
   "metadata": {},
   "source": [
    "# 5. Stopwords\n",
    "불용어 처리\n",
    "- 갖고 있는 데이터에서 유의미한 단어 토큰만을 선별하기 위해서는 큰 의미가 없는 단어 토큰을 제거하는 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2907a86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['데/NNB', '좀/MAG', '수/NNB', '등/NNB']\n",
    "\n",
    "def remove_stopword_text(text):\n",
    "    corpus = []\n",
    "    for sent in text:\n",
    "        modi_sent = []\n",
    "        for word in sent.split(' '):\n",
    "            if word not in stopwords:\n",
    "                modi_sent.append(word)\n",
    "        corpus.append(' '.join(modi_sent))\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fa6930",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2d75c92",
   "metadata": {},
   "source": [
    ">> 정리 << \n",
    "띄어쓰기 - py-spacing\n",
    "문장분리 - kss\n",
    "문장부호제거 - python의 re이용\n",
    "맞춤법 검사 - py-hanspell ( + 반복 이모티콘, 자모 정규화/외래어처리)\n",
    "형태소 분리 - konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87250862",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f2abbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5444851",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b92eba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72e62e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8244c10c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf556021",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
